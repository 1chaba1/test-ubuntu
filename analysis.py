# -*- coding: utf-8 -*-
"""Разведочный анализ данных.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1AavQmGyB0q_lVPU75pjosAZL6wT-g1Aq

Импортируем библиотеки, которые будем использовать для равзедочного анализа
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import polars as pl

"""Загружаем датасет abalone.csv с данными о молюсках с помощью библиотеки pandas"""

DATASET = 'https://raw.githubusercontent.com/aiedu-courses/eda_and_dev_tools/refs/heads/main/datasets/abalone.csv'
df = pd.read_csv(DATASET)
df.head()

"""``` Sex``` - Пол моллюска




```Length (Длина)``` - длина раковины в мм

```Diameter (Диаметр)``` - диаметр раковины в мм

```Height (Высота)``` - высота раковины в мм

```Whole weight (Общий вес)``` - полный вес в граммах

```Shucked weight (Вес мяса)``` - вес мяса без раковины

```Viscera weight (Вес внутренностей)``` - вес внутренних органов

```Shell weight (Вес раковины)``` - вес высушенной раковины

```Rings (Количество колец)``` - целое число, используется для определения возраста

Возраст моллюска = Rings + 1.5 года

Размер датасета и информация
"""

df.shape

df.info()

"""Проверим у категориальных признаков уникальные значения"""

df['Sex'].unique()

"""Заметим, что есть ```F``` и ```f```, это ошибка в данных, поэтому заменим f на F"""

df['Sex'] = df['Sex'].replace('f', 'F')

df['Sex'].unique()

"""Количество пропущенных значений"""

df.isnull().sum()

"""Заметим, что пропущенные значения есть в ```Diameter, Whole weight, Shell weight```"""

df.duplicated().sum()

"""Дубликатов нет

Расмотрим распределние данных в виде гисторамм
"""

plt.figure(figsize = (15, 8))

plt.subplot(2,4,1)
sns.histplot(df.Length, kde = True)
plt.xlabel('Длина раковины', labelpad=3)
plt.title('Распределение Длины')

plt.subplot(2,4,2)
sns.histplot(df.Diameter, kde = True)
plt.xlabel('Диаметр раковины', labelpad = 3)
plt.title('Распределение диаметра')

plt.subplot(2,4,3)
sns.histplot(df.Height, bins = 100, kde = True)
plt.xlim((0.0, 0.3))
plt.xlabel('Высота раковины')
plt.title('Распределение высоты раковины')

plt.subplot(2,4,4)
sns.histplot(df['Whole weight'], kde = True)
plt.xlabel('Полный вес')
plt.title('Распределение полного веса')

plt.subplot(2,4,5)
sns.histplot(df['Shucked weight'], kde = True)
plt.xlabel('Вес мяса без раковины', labelpad = 3)
plt.title('Распределение веса мяса без раковины')

plt.subplot(2,4,6)
sns.histplot(df['Viscera weight'], kde = True)
plt.xlabel('вес внутренних органов')
plt.title('Распеределение веса внутренностей')

plt.subplot(2,4,7)
sns.histplot(df['Shell weight'], kde = True)
plt.xlabel('Вес раковины')
plt.title('Распределение веса раковины')

plt.subplot(2,4,8)
sns.histplot(df.Rings, bins = 20,kde = True)
plt.xlabel('Количество колец')
plt.title('Распределение количества колец')


plt.tight_layout()
plt.show()

"""**ВЫВОД:** по графикам видно, что столбцы ```Length, Diameter, Height, Rings``` имеют нормальное распределение, у остальных столбцов большее количество значений приобладает на малых значениях измеряемой величины

Заполним пропущенные значения в зависимости от процента пропусков в столбце
"""

df.isnull().mean() * 100

"""Так как процентов пропусков меньше ~2.5%, заполним пропуски медиаными значениями столбцов"""

df['Diameter'] = df['Diameter'].fillna(df.Diameter.median())
df['Whole weight'] = df['Whole weight'].fillna(df['Whole weight'].median())
df['Shell weight'] = df['Shell weight'].fillna(df['Shell weight'].median())

df.isnull().sum()

col = [x for x in list(df.columns) if df[x].dtype != 'object' and x != 'Rings']

fig, axes = plt.subplots(3,3, figsize=(15, 8))

axes = axes.flatten()

for idx, name in enumerate(col):
  axes[idx].scatter(df[name], df.Rings)
  axes[idx].set_title(f"Связь между {name} и Rings")


axes_to_remove = []
for ax in fig.axes:
    if len(ax.lines) == 0 and len(ax.collections) == 0 and len(ax.patches) == 0:
        axes_to_remove.append(ax)


for ax in axes_to_remove:
    fig.delaxes(ax)


plt.tight_layout()
plt.show()

"""На основе графиков трудно сделать выводы о связи между выбранными переменными и целевой переменной, поэтому постоим матрицу корреляций

Рассмотрим матрицу корреляций по Пирсону
"""

corr = df.corr(numeric_only = True)
sns.heatmap(corr, cmap = 'mako', annot = True);

"""Видим, что все признаки имеют прямопропорциональную связь с целевой переменной. Самая большая связь с весом раковины моллюска"""

corr = df.corr(numeric_only = True, method ='spearman')
sns.heatmap(corr, cmap = 'mako', annot = True);

"""Для подтверждения вывода, воспользуемся также матрицей корреляции по Спирмену, она дает почти такие же результаты

Расчитывать корреляцию для категориальных признаков можно по ANOVA
"""

from scipy.stats import f_oneway

Data = []

for c1 in df.columns:
  for c2 in df.columns:
    if df[c1].dtype == 'object' and df[c2].dtype != 'object':
      GROUPCategory = df.groupby(c1)[c2].apply(list)
      ANOVAres = f_oneway(*GROUPCategory)

      if ANOVAres[1] >= 0.05:
        Data.append({'Category' : c1, 'Numerical' : c2, 'Is correlated' : 'No'})
      else:
        Data.append({'Category' : c1, 'Numerical' : c2, 'Is correlated' : 'Yes'})

ANOVARES = pd.DataFrame(Data)
display(ANOVARES)

"""Как видим статистические различия между единственной категориальной группой ```Sex``` и целевой переменной ```Rings``` присутствует

Рассмотрим распределение целевой переменной и сделаем выводы, как это будет влиять на модель машинного обучения
"""

sns.histplot(df.Rings, bins = 30,kde = True)
plt.title('Распределение целевой переменной Rings')
plt.show()

"""По графику видно, что распределение Rings - нормальное.

Так как распределение нормальное, ошибка после обучения нашей модели должна быть близка к минимальной

Создаем копию нашего датасета, только увеличиваем количество строк ~1000000
"""

df_big = df.copy()

for _ in range(260):
  df_big = pd.concat([df_big, df])

df_big.to_csv('abalone_big.csv', index = False)

df_big.shape, df.shape

"""Получаем размер увеличенного датасета 1090197 строк, в сравнении в исходном датасете 4177 строк

Сравним производительность двух библиотек для анализа данных ```Polars``` и ```Pandas```. Будем сравнивать по времени:
1) чтению файла , 2) фильтрации данных, 3) агрегации данных

------------------------
1) Чтение файла
"""

# Commented out IPython magic to ensure Python compatibility.
# # POLARS
# %%time
# 
# df_pl = pl.read_csv('abalone_big.csv')

# Commented out IPython magic to ensure Python compatibility.
# # Pandas
# %%time
# 
# df = pd.read_csv('abalone_big.csv')

"""Из результатов выполнения двух команд, видно то, что ```Polars``` немного быстрее, чем ```Pandas```, разница составляет не более 200-500 ms

--------------------------------------------------
2) Фильтрация данных

Найдем данные, где возраст (```Rings``` + 1.5 года) > среднего возраста и полный вес ```Whole weight``` > 1
"""

# Commented out IPython magic to ensure Python compatibility.
# #Pandas
# %%time
# 
# df[(df['Rings']+1.5 > np.mean(df['Rings'] + 1.5)) & (df['Whole weight'] > 1)]

# Commented out IPython magic to ensure Python compatibility.
# #POLARS
# %%time
# 
# df_pl.filter(((pl.col('Rings') + 1.5 > pl.col('Rings').mean() + 1.5))& (pl.col('Whole weight') > 1))

"""Как видно фильтрация отличается на 5-10 ms, где-то ```Pandas``` быстрее, а где-то ```Polars```

-----------------------------------------------------------
3) Агрегация данных

Сгруппируем данные по полу ```Sex```,найдем среднее значение полного веса ```Whole weight```и максимальноезначение по количеству колец моллюска ```Rings```
"""

# Commented out IPython magic to ensure Python compatibility.
# #Pandas
# %%time
# 
# df.groupby('Sex').agg({'Whole weight':'mean', 'Rings':'max'})

"""Для группировки с помощью ```Polars``` сначала воспользуемся прямой группировкий, а потом используем метод Lazy"""

# Commented out IPython magic to ensure Python compatibility.
# # Polars
# %%time
# 
# df_pl.group_by('Sex').agg([pl.mean('Whole weight'), pl.max('Rings')])

# Commented out IPython magic to ensure Python compatibility.
# # Polars - Lazy
# %%time
# 
# q = (
#     df_pl
#     .lazy()
#     .group_by(by = 'Sex')
#     .agg([
#         pl.col('Whole weight').mean()
#         , pl.col('Rings').max()
#     ])
# )
# 
#

# Commented out IPython magic to ensure Python compatibility.
# %%time
# 
# q.collect()

"""По результатам видно, что группировка у двух библиотек не сильно отличается по времени, но в библиотеки ```Polars``` есть полезный метод Lazy"""